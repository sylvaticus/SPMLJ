<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>0401 NeuralNetworksTheory · SPMLJ</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q39LHCRBB6"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-Q39LHCRBB6', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../index.html">SPMLJ</a></span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Index</a></li><li><span class="tocitem">Lessons</span><ul><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox"/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">INTRO - Introduction to the course, Julia and ML</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../00_-_INTRO_-_Introduction_julia_ml/0001-course_presentation.html">0001-course presentation</a></li><li><a class="tocitem" href="../00_-_INTRO_-_Introduction_julia_ml/0002-program.html">0002-program</a></li><li><a class="tocitem" href="../00_-_INTRO_-_Introduction_julia_ml/0003-introduction_to_Julia.html">0003-introduction to Julia</a></li><li><a class="tocitem" href="../00_-_INTRO_-_Introduction_julia_ml/0004-introduction_to_ml.html">0004-introduction to ml</a></li><li><a class="tocitem" href="../00_-_INTRO_-_Introduction_julia_ml/0005-installing_julia_and_git.html">0005-installing julia and git</a></li><li><a class="tocitem" href="../00_-_INTRO_-_Introduction_julia_ml/0006-q01-quiz_modules_packages_environment.html">0006-q01-quiz modules packages environment</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">JULIA1 - Basic Julia Programming</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../01_-_JULIA1_-_Basic_Julia_programming/0101-basic_syntax.html">0101-basic syntax</a></li><li><a class="tocitem" href="../01_-_JULIA1_-_Basic_Julia_programming/0101-q01-quiz_basic_syntax.html">0101-q01-quiz basic syntax</a></li><li><a class="tocitem" href="../01_-_JULIA1_-_Basic_Julia_programming/0102-types_and_objects.html">0102-types and objects</a></li><li><a class="tocitem" href="../01_-_JULIA1_-_Basic_Julia_programming/0103-predefined_types.html">0103-predefined types</a></li><li><a class="tocitem" href="../01_-_JULIA1_-_Basic_Julia_programming/0104-control_flow_and_functions.html">0104-control flow and functions</a></li><li><a class="tocitem" href="../01_-_JULIA1_-_Basic_Julia_programming/0105-custom_types.html">0105-custom types</a></li><li><a class="tocitem" href="../01_-_JULIA1_-_Basic_Julia_programming/0106-further_topics.html">0106-further topics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">JULIA2 - Scientific programming with Julia</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../02_-_JULIA2_-_Scientific_programming_with_Julia/0201-wrangling_data.html">0201-wrangling data</a></li><li><a class="tocitem" href="../02_-_JULIA2_-_Scientific_programming_with_Julia/0202-further_topics.html">0202-further topics</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">ML1 - Introduction to Machine Learning</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../03_-_ML1_-_Introduction_to_Machine_Learning/0301-MachineLearningMainIdeas.html">0301-MachineLearningMainIdeas</a></li><li><a class="tocitem" href="../03_-_ML1_-_Introduction_to_Machine_Learning/0302-perceptron.html">0302-perceptron</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox" checked/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">NN - Neural Networks</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href="0401_NeuralNetworksTheory.html">0401 NeuralNetworksTheory</a><ul class="internal"><li><a class="tocitem" href="#Motivations-and-types"><span>Motivations and types</span></a></li><li><a class="tocitem" href="#Feed-forward-neural-networks"><span>Feed-forward neural networks</span></a></li><li><a class="tocitem" href="#Convolutional-neural-networks"><span>Convolutional neural networks</span></a></li><li><a class="tocitem" href="#Recurrent-Neural-Networks-(RNNs)"><span>Recurrent Neural Networks (RNNs)</span></a></li></ul></li><li><a class="tocitem" href="0402_Implementing_neural_network_models.html">0402 Implementing neural network models</a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Lessons</a></li><li><a class="is-disabled">NN - Neural Networks</a></li><li class="is-active"><a href="0401_NeuralNetworksTheory.html">0401 NeuralNetworksTheory</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="0401_NeuralNetworksTheory.html">0401 NeuralNetworksTheory</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/sylvaticus/SPMLJ/blob/master/lessonsSources/04_-_NN_-_Neural_Networks/0401_NeuralNetworksTheory.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Neural-Networks-theory"><a class="docs-heading-anchor" href="#Neural-Networks-theory">0401 - Neural Networks - theory</a><a id="Neural-Networks-theory-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-Networks-theory" title="Permalink"></a></h1><p>While powerful, neural networks are actually composed of very simple units that are akin to linear classification or regression methods. Don&#39;t be afraid by their name and the metaphor with the human brain. Neural networks are really simple transformations of the data that flow from the input, through various layers, ending with an output.   That&#39;s their beauty ! Complex capabilities emerge from very simple units when we put them together. For example the capability to recognise not &quot;just&quot; objects in an image but also abstract concepts such as people&#39;s emotions or situations and environments.</p><p>We&#39;ll first describe them, and we&#39;ll later learn how to train a neural network from the data. Concerning the practical implementation in Julia, we&#39;ll not implement a complete neural network but rather only certain parts, as we will mostly deal with using them and apply them to different kinds of datasets.</p><h2 id="Motivations-and-types"><a class="docs-heading-anchor" href="#Motivations-and-types">Motivations and types</a><a id="Motivations-and-types-1"></a><a class="docs-heading-anchor-permalink" href="#Motivations-and-types" title="Permalink"></a></h2><p>When we studied the Perceptron algorithm, we noted how we can transform the original feature vector <span>$\mathbf{x}$</span> to a feature representation  <span>$\phi(\mathbf{x})$</span> that includes non-linear transformations, to still use linear classifiers for non-linearly separable datasets (we studied classification tasks but the same is true for regression ones). The &quot;problem&quot; is that this feature transformation is not learned from the data but is applied a priori, before using the actual machine learning (linear) algorithm. With neural networks instead, the feature transformation is endogenous to the learning (training) step.</p><p>We will see three kinds of neural networks:</p><ul><li><strong>Feed-forward Neural Networks</strong>, the simplest one where the inputs flow through a set of &quot;layers&quot; to reach an output. </li><li><strong>Convolutional Neural Networks (CNN)</strong>, where one or more of the layers is a &quot;convolutional&quot; layer. These are used mainly for image classification</li><li><strong>Recurrent Neural Networks (RNN)</strong>, where the input arrives not only at the beginning but also at each layer. RNNs are used to learn sequences of data</li></ul><h2 id="Feed-forward-neural-networks"><a class="docs-heading-anchor" href="#Feed-forward-neural-networks">Feed-forward neural networks</a><a id="Feed-forward-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Feed-forward-neural-networks" title="Permalink"></a></h2><h3 id="Description"><a class="docs-heading-anchor" href="#Description">Description</a><a id="Description-1"></a><a class="docs-heading-anchor-permalink" href="#Description" title="Permalink"></a></h3><p>In <strong>deep forward neural networks</strong>, neural network units are arranged in <strong>layers</strong>, from the <em>input layer</em>, where each unit holds the input coordinate, through various <em>hidden layer</em> transformations, until the actual <em>output</em> of the model:</p><p><img src="https://raw.githubusercontent.com/sylvaticus/SPMLJ/main/lessonsSources/04_-_NN_-_Neural_Networks/imgs/feedforwardNNChart.png" alt="Neural network scheme"/></p><p>More in detail, considering a single <em>dense</em> neuron (in the sense that is connected with <em>all</em> the previous layer&#39;s neurons or with the input layer), we have the following figure:</p><p><img src="https://raw.githubusercontent.com/sylvaticus/SPMLJ/main/lessonsSources/04_-_NN_-_Neural_Networks/imgs/singleNeuron.png" alt="Single neuron"/></p><p>where:</p><ul><li><p class="math-container">\[x\]</p>is a two-dimensional input, with <span>$x_1$</span> and <span>$x_2$</span> being the two dimensions of our input data (they could equivalently be the outputs of a previous 2 neurons layers)</li><li><p class="math-container">\[w\]</p>are the <em>weigths</em> that are applied to <span>$x$</span> plus a constant term (<span>$w_0$</span>). <strong>These are the parameter we will want to learn with our algorithm</strong>. <span>$f$</span> is a function (often non-linear) that is applied to <span>$w_0 + x_1w_1 + x_2w_2$</span> to define the output of the neuron</li></ul><p>The output of the neuron can be the output of our neural network or it can be the input of a further layer.</p><p>In Julia we can implement a layer of neurons and its predictions very easily (although implementing the learning of the weights is a bit more complex):</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using LinearAlgebra</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; mutable struct DenseLayer
           wb::Array{Float64,1} # weights with reference to the bias (will be learned from data)
           wi::Array{Float64,2} # weigths with reference to the input (will be learned from data)
           f::Function          # the activation function of each neuron (chosen from the modeller)
       end</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; function forward(m,x) # The predictions  - or &quot;forward&quot; to the next layer
             return m.f.(m.wb .+ m.wi * x)
       end</code><code class="nohighlight hljs ansi" style="display:block;">forward (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; (nI,nO) = 3,2 # Number of nodes in input and in outputs of this layer</code><code class="nohighlight hljs ansi" style="display:block;">(3, 2)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; layer = DenseLayer(rand(nO),rand(nO,nI),tanh)</code><code class="nohighlight hljs ansi" style="display:block;">Main.DenseLayer([0.9886658021667125, 0.25175894240863705], [0.22882635231244608 0.5353902253325116 0.8469695432047211; 0.3374269685250667 0.6871349196845488 0.8373112676893617], tanh)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; x = zeros(nI)</code><code class="nohighlight hljs ansi" style="display:block;">3-element Vector{Float64}:
 0.0
 0.0
 0.0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; y = forward(layer,x)</code><code class="nohighlight hljs ansi" style="display:block;">2-element Vector{Float64}:
 0.7567928440732454
 0.24657138069414725</code></pre><p>Let&#39;s specific a bit of terminology concerning Neural Networks:</p><ul><li>The individual computation units of a layer are known as <strong>nodes</strong> or <strong>neurons</strong>.</li><li><strong>Width_l</strong> (<em>of the layer</em>) is the number of units in that specific layer <span>$l$</span></li><li><strong>Depth</strong> (<em>of the architecture</em>) is number of layers of the overall transformation before arriving to the final output</li><li>The <strong>weights</strong> are denoted with <span>$w$</span> and are what we want the algorithm to learn.</li><li>Each node&#39;s <strong>aggregated input</strong> is given by <span>$z = \sum_{i=1}^d x_i w_i + w_0$</span> (or, in vector form, <span>$z = \mathbf{x} \cdot \mathbf{w} + w_0$</span>, with <span>$z \in \mathbb{R}, \mathbf{x} \in \mathbb{R}^d, \mathbf{w} \in \mathbb{R}^d$</span>) and <span>$d$</span> is the width of the previous layer (or the input layer)</li><li>The output of the neuron is the result of a non-linear transformation of the aggregated input called <strong>activation function</strong> <span>$f = f(z)$</span></li><li>A <strong>neural network unit</strong> is a primitive neural network that consists of only the “input layer&quot;, and an output layer with only one output.</li><li><strong>hidden layers</strong> are the layers that are not dealing directly with the input nor the output layers </li><li><strong>Deep neural networks</strong> are neural network with at least one hidden layer</li></ul><p>While the weights will be learned, the width of each layer, the number of layers and the activation functions are all elements that can be tuned as hyperparameters of the model, although there are some more or less formal &quot;rules&quot;:</p><ul><li>the input layer is equal to the dimensions of the input data;</li><li>the output layer is equal to the dimensions of the output data. This is typically a scalar in a regression task, but it is equal to the number of categories in a multi-class classification, where each &quot;output dimension&quot; will be the probability associated with that given class;</li><li>the number of hidden layers reflects our judgment on how many &quot;levels&quot; we should decompose our input to arrive at the concept expressed in the label <span>$y$</span> (we&#39;ll see this point dealing with image classification and convolutional networks). Often 1-2 hidden layers are enough for classical regression/classification;</li><li>the number of neurons should give some &quot;flexibility&quot; to the architecture without exploding too much the number of parameters. A rule of thumb is to use about 20% more neurons than the input dimension. This is often fine-tuned using cross-validation as it risks leading to overfitting;</li><li>the activation function of the layers, with the exclusion of the last one, is chosen between a bunch of activation functions, nowadays almost always the <em>Rectified Linear Unit</em> function, aka <code>relu</code>, defined as <code>relu(x) = max(0,x)</code>. The <code>relu</code> function has the advantage to add non-linearity to the transformation while remaining fast to compute (including the derivative) and limiting the problem of vanishing or exploding the gradient (we&#39;ll see this aspect when dealing with the actual algorithm to obtain the weights). Another common choice is <code>tanh()</code>, the hyperbolic tangent function, that maps with an &quot;S&quot; shape the real line to the interval [-1,1].</li><li>the activation function of the last layer depends on the nature of the labels we want the network to compute: if these are positive scalars we can use also here the <code>relu</code>, if we are doing a binary classification we can use the <code>sigmoid</code> function defined as <code>sigmoid(x) = 1/(1+exp(-x))</code> whose output is in the range [0,1] and which we can interpret as the probability of the class that we encode as <code>1</code>. If we are doing a multi-class classification we can use the <code>softmax</code> function whose output is a PMF of probabilities for each class. It is defined as <span>$softmax(x,k) = \frac{e^{x_k}}{\sum_{j=1}^{K} e^{x_j}}$</span> where <span>$x$</span> is the input vector, <span>$K$</span> its length and k the specific position of the class for which we want to retrieve its &quot;probability&quot;.</li></ul><p>Let&#39;s now make an example of a single layer, single neuron with a 2D input <code>x=[2,4]</code>, weights <code>w=[2,1]</code>, <code>w₀ = 2</code> and activation function <code>f(x)=sin(x)</code>.</p><p>In such a case, the output of our network is <code>sin(2+2*2+4*1)</code>, i.e. -0.54. Note that with many neurons and many layers this becomes essentially (computationally) a problem of matrix multiplications, but matrix multiplication is easily parallelisable by the underlying BLAS/LAPACK libraries or, even better, by using GPU or TPU hardware, and running neural networks (and computing their gradients) is at the core of the demand for GPU computation.  </p><p>Let&#39;s now assume that the true label that we know to be associated with our <span>$x$</span> is <code>y=-0.6</code>.</p><p>Out (basic) network did pretty well, but still did an <em>error</em>: -0.6 is not -0.54. The last element of a neural network is indeed to define an error metric (the <strong>loss function</strong>) between the output computed by the neural network and the true label. Commonly used loss functions are the squared l-2 norm (i.e. <span>$\epsilon = \mid \mid \hat y - y \mid\mid ^2$</span>) for regression tasks and cross-entropy (i.e. <span>$\epsilon = - \sum_d p_d  * log(\hat p_d)$</span>) for classification jobs.</p><p>Before moving to the next section, where we will study how to put everything together and learn how to train the neural network in order to reduce this error, let&#39;s first observe that neural networks are powerful tools that can work on many sorts of data, but they require however the input to be encoded in a numerical form, as the computation is strictly numerical. If I have a categorical variable, for example, I&#39;ll need to encode it expanding it to a set of dimensions where each dimension represent a single class and I encode with an indicator function if my record is that particular class or not. This is the most simple form of encoding and takes the name of <em>one hot encoding</em>:</p><p><img src="https://raw.githubusercontent.com/sylvaticus/SPMLJ/main/lessonsSources/04_-_NN_-_Neural_Networks/imgs/onehotencoding.png" alt="One-hot encoding"/></p><p>Note in the figure that using all the three columns leads to linearly dependency, and while, yes, we could save some resources by using only two columns instead of three, this is not a fundamental problem like it would be in statistical analysis. </p><h3 id="Training-of-a-feed-forward-neural-network"><a class="docs-heading-anchor" href="#Training-of-a-feed-forward-neural-network">Training of a feed-forward neural network</a><a id="Training-of-a-feed-forward-neural-network-1"></a><a class="docs-heading-anchor-permalink" href="#Training-of-a-feed-forward-neural-network" title="Permalink"></a></h3><h4 id="Gradient-and-learning-rate"><a class="docs-heading-anchor" href="#Gradient-and-learning-rate">Gradient and learning rate</a><a id="Gradient-and-learning-rate-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-and-learning-rate" title="Permalink"></a></h4><p>We now need a way to <em>learn</em> the parameters from the data, and a common way is to try to reduce the contribution of the individual parameter to the error made by the network. We need first to find the link between the individual parameter and the output of the loss function, that is how the error change when we change the parameter. But this is nothing else than the derivative of the loss function with respect to the parameter. In our simple one-neuron example above we have the parameters directly appearing in the loss function. Considering the squared error as lost we have <span>$\epsilon = (y - sin(w_0 + w_1 x_1 + w_2 x_2))^2$</span>. If we are interested in the <span>$w_1$</span> parameter we can compute the derivate of the error with respect to it using the chain rule as <span>$\frac{\partial\epsilon}{\partial w_1} = 2*(y - sin(w_0 + w_1 x_1 + w_2 x_2)) * - cos(w_0 + w_1 x_1 + w_2 x_2) * x_1$</span>.</p><p>Numerically, we have: <span>$\frac{\partial\epsilon}{\partial w_1} = 2(-0.6-sin(2+4+4)) * -cos(2+4+4) * 2 = -0.188$</span> If I increase <span>$w_1$</span> of 0.01, I should have my error moving of <span>$-0.01*0.188 = -0.0018$</span>. Indeed, if I compute the original error I have <span>$\epsilon^{t=0} = 0.00313$</span>, but after having moved <span>$w_1$</span> to 2.01, the output of the neural network chain would now be <span>$\hat y^{t=1} = 0.561$</span> and its error lowered to <span>$\epsilon^{t=1} =  0.00154$</span>. The difference is <span>$0.00159$</span>, slighly lower in absolute terms than what we computed with the derivate, <span>$0.0018$</span>. The reason, of course, is that the derivative is a concept at the margin, when the step tends to zero.</p><p>We should note a few things:</p><ul><li>the derivate depends on the level of <span>$w_1$</span>. &quot;zero&quot; is almost always a bad starting point (as the derivatives of previous layers will be zero). Various initialisation strategies are used, but all involve sampling randomly the initial parameters under a certain range</li><li>the derivate depends also on the data on which we are currently operating, <span>$x$</span> and <span>$y$</span>. If we consider different data we will obtain different derivates</li><li>while extending our simple example to even a few more layers would seem to make the above exercise extremely complex, it remains just an application of the chain rule, and we can compute the derivatives efficiently by making firstly a <em>forward passage</em>, computing (and storing) the values of the chain at each layer, and then making a <em>backward passage</em> by computing (and storing) the derivatives with the chain rule backwards from the last to the first layer</li><li>the fact that the computation of the derivates for a layer includes the <em>multiplication</em> of the derivates for all the other layers means that if these are very small (big) the overall derivate may vanish (explode). This is a serious problem with neural networks and one of the main reasons why simple activation functions such as the <code>relu</code> are preferred.</li></ul><p>The derivate(s) of the error with respect to the various parameters is called the <em>gradient</em>.</p><p>If the gradient with respect to a parameter is negative, like in our example, it means that if we slightly <em>increase</em> the parameter we will obtain a lower error. On the opposite, if it is positive, if we slightly <em>reduce</em> the parameter we should find a lower error.</p><p>A gradient-descent based algorithm can hence be used to look iteratively for the minimum error by moving <em>against</em> the gradient with a certain step. The most basic algorithm is then <span>$w_i^t = w_i^{t-1} - \frac{\partial\epsilon^{t-1}}{\partial w_1^{t-1}} * \lambda$</span> where <span>$\lambda$</span> is the step that you are willing to make against the gradient, also known as <em>learning rate</em>. Note in the example above that if instead of moving the parameter <span>$w_1$</span> of <span>$0.01$</span> we would have increased it of <span>$0.1$</span>, we would have increased the error to <span>$0.00997$</span> instead of reducing it. This highlights the problem to use a good learning rate (see next Figure): a too-small learning rate would make the learning slow and with the risk to get trapped in a local minimum instead of a global one. Conversely, a too-large learning rate would risk causing the algorithm to diverge.</p><p><img src="https://raw.githubusercontent.com/sylvaticus/SPMLJ/main/lessonsSources/04_-_NN_-_Neural_Networks/imgs/learningRateEffect.png" alt="Learning rate effect"/></p><p>So, the learning rate is also a hyper-parameter to calibrate, although some modern gradient descent variations, like the ADAptive Moment estimation (ADAM) optimisation algorithm, tend to self_tune themselves and we rarely need to calibrate the default values.</p><h4 id="Batches-and-Stochastic-gradient-descent"><a class="docs-heading-anchor" href="#Batches-and-Stochastic-gradient-descent">Batches and Stochastic gradient descent</a><a id="Batches-and-Stochastic-gradient-descent-1"></a><a class="docs-heading-anchor-permalink" href="#Batches-and-Stochastic-gradient-descent" title="Permalink"></a></h4><p>We already note that the computation of the gradient depends on the data levels. We can then move between two extremes: on one extreme we compute the gradient as the average of those computed on all data points and we apply the optimisation algorithm to this average. On the other extreme, we sample randomly record by record and, at each record, we move the parameter. The compromise is to partition the data in a set of <em>batches</em>, compute the average gradient of each batch and at each time update the parameter with the optimisation algorithm. The &quot;one record at the time&quot; is the slowest approach, but also is very sensitive to the presence of outliers. The &quot;take the average of all the data&quot; approach is faster in running a certain epoch, but it takes longer to converge (i.e. it requires more epochs, the number of times we pass through the whole training data). It also requires more memory, as we need to store the gradients with respect to all records. So, the &quot;batch&quot; approach is a good compromise, and we normally set the batch number to a multiplier of the number of threads in the machine performing the training, as this step is often parallelised, and it represents a further parameter that we can cross-validate. When we sample the records (individually or in batch) before running the optimisation algorithm we speak of <em>stochastic gradient descent</em>.</p><h2 id="Convolutional-neural-networks"><a class="docs-heading-anchor" href="#Convolutional-neural-networks">Convolutional neural networks</a><a id="Convolutional-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Convolutional-neural-networks" title="Permalink"></a></h2><h3 id="Motivations"><a class="docs-heading-anchor" href="#Motivations">Motivations</a><a id="Motivations-1"></a><a class="docs-heading-anchor-permalink" href="#Motivations" title="Permalink"></a></h3><p>Despite typically classified separately, convolutional neural networks are essentially feed-forward neural networks with the only specification that one or more layers are convolutional. These layers are very good in recognising patterns within data with many dimensions, like spatial data or images, where each pixel can be thought a dimension of a single record. In both cases, we could use &quot;normal&quot; feed-forward neural networks, but convolutional layers offer two big advantages:</p><ol><li><em>They require much fewer parameters</em>.</li></ol><p>Convolutional neurons are made of small <em>filters</em> (or <em>kernels</em>) (typically 3x3 or 5x5) where the same weight convolves across the image. Conversely, if we would like to process with a dense layer a mid-size resolution image of <span>$1000 \times 1000$</span> pixels, each layer would need a weight matrix connecting all these 10^6 pixels in input with 10^6 pixel in output, i.e. 10^12 weights</p><ol><li><em>They can extend globally what they learn &quot;locally&quot;</em></li></ol><p>If we train a feed-forward network to recognise cars, and it happens that our training photos have the cars all in the bottom half, then the network would not recognise a car in the top half, as these would activate different neurons. Instead, convolutional layers can learn independently on where the individual features apply.</p><h3 id="Description-2"><a class="docs-heading-anchor" href="#Description-2">Description</a><a class="docs-heading-anchor-permalink" href="#Description-2" title="Permalink"></a></h3><p>In these networks, the layer <span>$l$</span> is obtained by operating over the image at layer <span>$l-1$</span> a small <strong>filter</strong> (or <strong>kernel</strong>) that is slid across the image with a step of 1 (typically) or more pixels at the time. The step is called <strong>stride</strong>, while the whole process of sliding the filter throughout the whole image can be mathematically seen as a <strong>convolution</strong>.</p><p>So, while we slide the filter, at each location of the filter, the output is composed of the dot product between the values of the filter and the corresponding location in the image (both vectorised), where the values of the filters are the weights that we want to learn, and they remain constant across the sliding. If our filter is a <span>$10 \times 10$</span> matrix, we have only 100 weights to learn by layer (plus one for the offset). Exactly as for feedforward neural networks, then the dot product is passed through an activation function, here typically the <code>ReLU</code> function (<span>$max(0,x)$</span>) rather than <code>tanh</code>:</p><p><img src="https://raw.githubusercontent.com/sylvaticus/SPMLJ/main/lessonsSources/04_-_NN_-_Neural_Networks/imgs/convolutionalFilter.png" alt="Convolutional filter"/></p><h3 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h3><p>For example, given an image <span>$x = \begin{bmatrix} 1 &amp; 1 &amp; 2 &amp; 1 &amp; 1 \\
3 &amp; 1 &amp; 4 &amp; 1 &amp; 1 \\
1 &amp; 3 &amp; 1 &amp; 2 &amp; 2 \\
1 &amp; 2 &amp; 1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 2 &amp; 1 &amp; 1 \\
\end{bmatrix}$</span> and filter weights <span>$w = \begin{bmatrix}  1 &amp; -2 &amp; 0 \\
 1 &amp;  0 &amp; 1 \\
-1 &amp;  1 &amp; 0 \\
\end{bmatrix}$</span>, then the output of the filter <span>$z$</span> would be <span>$\begin{bmatrix}  8 &amp; -3 &amp; 6 \\
 4 &amp;  -3 &amp; 5 \\
-3 &amp;  5 &amp; -2 \\
\end{bmatrix}$</span>. For example, the element of this matrix <span>$z_{2,3} = 5$</span> is the result of the sum of the scalar multiplication between <span>$x^\prime = \begin{bmatrix}  4 &amp;  1 &amp; 1 \\
 1 &amp;  2 &amp; 2 \\
 1 &amp;  1 &amp; 1 \\
\end{bmatrix}$</span> and <span>$w$</span>.</p><p>Finally, the output of the layer would be (using ReLU) <span>$\begin{bmatrix}  8 &amp; 0 &amp; 6 \\
 4 &amp;  0 &amp; 5 \\
 0 &amp;  5 &amp; 0 \\
\end{bmatrix}$</span>.</p><p>We can run the following snippet to make the above computations:</p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; ReLU(x) = max(0,x)</code><code class="nohighlight hljs ansi" style="display:block;">ReLU (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; x = [1 1 2 1 1;
               3 1 4 1 1;
               1 3 1 2 2;
               1 2 1 1 1;
               1 1 2 1 1]</code><code class="nohighlight hljs ansi" style="display:block;">5×5 Matrix{Int64}:
 1  1  2  1  1
 3  1  4  1  1
 1  3  1  2  2
 1  2  1  1  1
 1  1  2  1  1</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; w = [ 1 -2  0;
               1  0  1;
               -1  1  0]</code><code class="nohighlight hljs ansi" style="display:block;">3×3 Matrix{Int64}:
  1  -2  0
  1   0  1
 -1   1  0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; (xr,xc) = size(x)</code><code class="nohighlight hljs ansi" style="display:block;">(5, 5)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; (wr,wc) = size(w)</code><code class="nohighlight hljs ansi" style="display:block;">(3, 3)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; z = [sum(x[r:r+wr-1,c:c+wc-1] .* w) for c in 1:xc-wc+1 for r in 1:xr-wr+1] # Julia is column major</code><code class="nohighlight hljs ansi" style="display:block;">9-element Vector{Int64}:
  8
  4
 -3
 -3
 -3
  5
  6
  5
 -2</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; u = ReLU.(z)</code><code class="nohighlight hljs ansi" style="display:block;">9-element Vector{Int64}:
 8
 4
 0
 0
 0
 5
 6
 5
 0</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; final = reshape(u, xr-wr+1, xc-wc+1)</code><code class="nohighlight hljs ansi" style="display:block;">3×3 Matrix{Int64}:
 8  0  6
 4  0  5
 0  5  0</code></pre><p>You can notice that by applying the filter we obtain a dimensionality reduction. This reduction depends on both the dimension of the filter and the stride (sliding step). In order to avoid this, padding of one or more rows/columns can be applied to the image to preserve in the output the same dimension of the input (in the above example padding of one row and one column on both sides would suffice). Typically the padded cells are given a value of zero so not to contribute anything when they are included in the dot product computed by the filter.</p><p>To determine the spatial size in the output of a filter (<span>$O$</span>), given the input size (<span>$I$</span>), the filter size (<span>$F$</span>), the stride (<span>$S$</span>) and the eventual padding (<span>$P$</span>) we can use the following simple formula:</p><p class="math-container">\[O_d = 1 + (I_d+2*P_d-F_d)/S\]</p><p>From it, we can also find the padding needed to obtain a certain output size as: <span>$P_d = ((O_d-1)S_d-I_d+F_d)/2$</span></p><p>Where the <span>$d$</span> index accounts for the (extremely unusual) case where one of the parameters is not a square matrix so that for example an image has different vertical and horizontal resolutions.</p><p>Because the weights of the filters are the same, it doesn&#39;t really matter where the object is learned, in which part of the image. With convolutional layers, we have <em>translational invariance</em> as the same filter is passed over the entire image. Therefore, it will detect the patterns regardless of their location.</p><p>Still, it is often convenient to operate some <strong>data augmentation</strong> to the training set, that is to add slightly modified images (rotated, mirrored..) in order to improve this translational invariance.</p><h3 id="Considering-multiple-filters-per-layer"><a class="docs-heading-anchor" href="#Considering-multiple-filters-per-layer">Considering multiple filters per layer</a><a id="Considering-multiple-filters-per-layer-1"></a><a class="docs-heading-anchor-permalink" href="#Considering-multiple-filters-per-layer" title="Permalink"></a></h3><p>Typically, one single layer is formed by applying multiple filters, not just one. This is because we want to learn different kinds of features. For example in an image one filter will specialize to catch vertical lines, the other obliques ones, and maybe another filter different colours.</p><p><img src="https://raw.githubusercontent.com/sylvaticus/SPMLJ/main/lessonsSources/04_-_NN_-_Neural_Networks/imgs/convolutionalLayerOutputs.png" alt="Set of different convolutional filters outputs"/></p><p>Convolutional filters outputs on the first layer (filters are of size 11x11x3 and are applied across input images of size 224x224x3). Source: <a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">Krizhevsky et Oth. (2012), &quot;ImageNet Classification with Deep Convolutional Neural Networks&quot;</a></p><p>So in each layer we map the output of the previous layer (or the original image in case of the first layer) into multiple feature maps where each feature map is generated by a little weight matrix, the filter, that defines the little classifier that&#39;s run through the original image to get the associated feature map. Each of these feature maps defines a channel for information and we can represent it as a third dimension to form a &quot;volume&quot;, where the depth is given by the different filters:</p><p><img src="https://raw.githubusercontent.com/sylvaticus/SPMLJ/main/lessonsSources/04_-_NN_-_Neural_Networks/imgs/convolutionalLayer.png" alt="Convolutional layer"/></p><p>In the image above the input layer has size (4,4,4) and the output layer has size (3,3,2), i.e. 2 &quot;independent&quot; filters of size (2,2).</p><p>For square layers, each filter has <span>$F^2 \times D_{l-1} + 1$</span> parameters where <span>$D_{l-1}$</span> is the dimensions (&quot;depth&quot;) of the previous layer, so the total number of parameters per layer is <span>$(F^2 \times D_{l-1} + 1) * D_l$</span>.</p><p>For computational reasons, the number of filters per layer <span>$D_l$</span> is normally a power of 2. </p><p>This representation allows remaining consistent with the input that can as well be represented as a volume. For images, the depth is usually given by 3 layers representing the values in terms of RGB colours.</p><h3 id="Pool-layers"><a class="docs-heading-anchor" href="#Pool-layers">Pool layers</a><a id="Pool-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Pool-layers" title="Permalink"></a></h3><p>A further way to improve translational invariance, but also have some dimensionality reduction, is called <strong>pooling</strong> and is implemented by adding a layer with a filter whose output is the <code>max</code> of the corresponding area in the input (or, more rarely, the average). Note that this layer would have no weights to learn! With pooling we start separating what is in the image from where it is in the image, that is, pooling does a fine-scale, local translational invariance, while convolution does more a large-scale one.</p><p>Keeping the output of the above example as input, a pooling layer with a <span>$2 \times 2$</span> filter and a stride of 1 would result in <span>$\begin{bmatrix}  8 &amp; 6 \\
 5 &amp; 5 \\
\end{bmatrix}$</span>.</p><h3 id="Convolutional-networks-conclusions"><a class="docs-heading-anchor" href="#Convolutional-networks-conclusions">Convolutional networks conclusions</a><a id="Convolutional-networks-conclusions-1"></a><a class="docs-heading-anchor-permalink" href="#Convolutional-networks-conclusions" title="Permalink"></a></h3><p>We can then combine these convolutions, looking for features, and pooling, compressing the image a little bit, forgetting the information of where things are, but maintaining what is there.</p><p>In a typical CNN, these convolutional and pooling layers are repeated several times, where the initial few layers typically would capture the simpler and smaller features, whereas the later layers would use information from these low-level features to identify more complex and sophisticated features, like characterisations of a scene. The learned weights would hence specialise across the layers in a sequence like  edges -&gt; simple parts-&gt; parts -&gt; objects -&gt; scenes.</p><p>These layers are finally followed by some &quot;normal&quot;, &quot;fully connected&quot; layers (like in &quot;normal&quot; feed-forward neural networks) and a final <code>softmax</code> layer indicating the probability that each image represents one of the possible categories (there could be thousands of them).</p><p>The best network implementations are tested in so-called &quot;competitions&quot;, like the yearly ImageNet context.</p><p>Note that we can train these networks exactly like for feedforward NN, defining a loss function and finding the weights that minimise the loss function. In particular, we can apply the stochastic gradient descendent algorithm (with a few tricks based on getting pairs of image and the corresponding label), where the gradient with respect to the various parameters (weights) is obtained by backpropagation.</p><h2 id="Recurrent-Neural-Networks-(RNNs)"><a class="docs-heading-anchor" href="#Recurrent-Neural-Networks-(RNNs)">Recurrent Neural Networks (RNNs)</a><a id="Recurrent-Neural-Networks-(RNNs)-1"></a><a class="docs-heading-anchor-permalink" href="#Recurrent-Neural-Networks-(RNNs)" title="Permalink"></a></h2><h3 id="Motivations-2"><a class="docs-heading-anchor" href="#Motivations-2">Motivations</a><a class="docs-heading-anchor-permalink" href="#Motivations-2" title="Permalink"></a></h3><p>Recurrent neural networks are used to learn <em>sequences</em> of data. A &quot;sequence&quot; is characterised by the fact that each element may depend not only on the features in place at time <span>$t$</span>, but also from lagged features or lagged values of the sequence (we use here the time dimension just for simplicity. Of course, a sequence can be defined on any dimension). And here comes the problem: we could always consider lagged features or sequence values as further dimensions at time <span>$t$</span> and use a &quot;standard&quot; feed-forward network. For example we could consider values at time <span>$t-1$</span>, those at time <span>$t-2$</span> and those at time <span>$t-3$</span>. But, again, we would be doing &quot;manual&quot; feature engineering, similar to the way we can introduce non-linear feature transformation and use linear classifiers. But we want this to be learned by the algorithm. We want the model to learn how much of the history retain to predict the next element of the sequence, and which elements &quot;deserve&quot; to be kept in memory (to be used for predictions) even if far away in the sequence steps.</p><h3 id="Description-3"><a class="docs-heading-anchor" href="#Description-3">Description</a><a class="docs-heading-anchor-permalink" href="#Description-3" title="Permalink"></a></h3><p>There are a few differences with feed-forward neural networks:</p><ul><li>the input doesn&#39;t arrive only at the beginning of the chain, but at each layer (each input being an element of the sequence)</li><li>each RNN layer processes, using learnable parameters, the input corresponding to its layer, together the input coming from the previous layers (called the state)</li><li>these weights are shared for the various RNN layers across the sequence</li></ul><p>Note that you can interpret a recurrent network equivalently like being formed by different layers on each element of the sequence (but with shared weights) or like a single, evolving, layer that calls itself recursively. Note also the similarities with convolutional networks: there we have a filter than convolves along the image, keeping the weigths constant across the convolution, here we have a recurrent network that also &quot;filter&quot; the whole sequence and learn some shared weigths.  </p><p>To implement a recurrent neural network we can adapt our code above to include the state: </p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; mutable struct RNNLayer
           wb::Array{Float64,1} # weights with reference to the bias
           wi::Array{Float64,2} # weigths with reference to the input
           ws::Array{Float64,2} # weigths with reference to the state
           f::Function
       end</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; (nI,nO)  = 3,2</code><code class="nohighlight hljs ansi" style="display:block;">(3, 2)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; relu(x)  = max(0,x)</code><code class="nohighlight hljs ansi" style="display:block;">relu (generic function with 1 method)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; rnnLayer = RNNLayer(rand(nO),rand(nO,nI),rand(nO,nO),relu)</code><code class="nohighlight hljs ansi" style="display:block;">Main.RNNLayer([0.009793771133059681, 0.2451738630886735], [0.7587955942199099 0.8883291034342745 0.3984675606303608; 0.8366312917809341 0.6807782629269769 0.20984799792849484], [0.09607171363771372 0.6863957018710027; 0.5394538922089241 0.21081248135790054], Main.relu)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; function forward(m,x,s)
           return m.f.(m.wb .+ m.wi * x .+ m.ws * s)
       end</code><code class="nohighlight hljs ansi" style="display:block;">forward (generic function with 2 methods)</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; x,s = zeros(nI),zeros(nO)</code><code class="nohighlight hljs ansi" style="display:block;">([0.0, 0.0, 0.0], [0.0, 0.0])</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; s = forward(rnnLayer,x,s)</code><code class="nohighlight hljs ansi" style="display:block;">2-element Vector{Float64}:
 0.009793771133059681
 0.2451738630886735</code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; s = forward(rnnLayer,x,s)  # The state change even if x remains constant</code><code class="nohighlight hljs ansi" style="display:block;">2-element Vector{Float64}:
 0.17902096134396348
 0.3021428614876314</code></pre><p>The code above is the simplest implementation of a Recurrent Neural Network (or at least of its forward passage). In practice, the state is often memorised as part of the layer structure so its usage in most neural network libraries is similar to a &quot;normal&quot; feed-forward layer <code>forward(layer,x)</code>.</p><h3 id="Usage:-sequence-to-one"><a class="docs-heading-anchor" href="#Usage:-sequence-to-one">Usage: sequence-to-one</a><a id="Usage:-sequence-to-one-1"></a><a class="docs-heading-anchor-permalink" href="#Usage:-sequence-to-one" title="Permalink"></a></h3><p>RNNs can be used to characterise a sequence, like in sentiment analysis to predict the overall attitude (positive or negative) of a text or the language in which the text is written. In these cases, the RNN task is to <em>encode</em> the sequence in a vector format (the final state) and this is fed to a further part of the chain whose task is to <em>decode</em> according to the task required. Note that the parameters for both tasks are learned jointly. The scheme is as follow:</p><p><img src="https://raw.githubusercontent.com/sylvaticus/SPMLJ/main/lessonsSources/04_-_NN_-_Neural_Networks/imgs/sequenceToOne.png" alt="Sequence-to-one scheme"/></p><p>Training in this scenario implies starting the model from an initial state (normally a zero-vector) and some random weights,  and  &quot;feeding&quot; the model with one item at a time until the sequence ends. At this time the final state is decoded to an overall output that is compared to the &quot;true&quot; y.  From here the backward passage is made in a similar way that in feed-forward networks so that the &quot;contribution&quot; of each weight to the errors can be assessed and the weights adjusted</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>While weights are progressively adjusted across the training samples, the state of the network should be reset at each new sequence sample.</p></div></div><h3 id="Usage:-sequence-to-sequence"><a class="docs-heading-anchor" href="#Usage:-sequence-to-sequence">Usage: sequence-to-sequence</a><a id="Usage:-sequence-to-sequence-1"></a><a class="docs-heading-anchor-permalink" href="#Usage:-sequence-to-sequence" title="Permalink"></a></h3><p>Another scenario is when we want the RNN to <em>replicate</em> some sequence pattern, like in next word, next note or next price predictions. In this case, we are interested in all the elements of the sequence and not only in the final state of the sequence.  The decoding part happens hence at each step of the sequence and the resulting <span>$\hat y_i$</span> is compared with the true <span>$y_i$</span>, with the resulting loss used to train the weights:</p><p><img src="https://raw.githubusercontent.com/sylvaticus/SPMLJ/main/lessonsSources/04_-_NN_-_Neural_Networks/imgs/sequenceToSequence.png" alt="Sequence-to-sequence scheme"/></p><h3 id="Gated-networks"><a class="docs-heading-anchor" href="#Gated-networks">Gated networks</a><a id="Gated-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Gated-networks" title="Permalink"></a></h3><p>While theoretically RNN can &quot;learn&quot; the importance of features across indeterminately long sequence steps, in practice the fact of continuing multiplicating the status across the varius elements of the sequence makes the problem of vanishing gradient even stronger for them. New contributions have hence been proposed with a &quot;gating&quot; system that &quot;learns&quot; what to store in memory (in the sequence state) and what to &quot;forget&quot;. At the time of writing the most used approach is the <em>Long short-term memory (LSTM)</em>. While internally more complex due to the presence of the gates and of several different states (<em>hidden</em> and <em>visible</em> in LSTM), LSTM networks are operationally used exactly in the same ways as the RNN networks described above.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../03_-_ML1_-_Introduction_to_Machine_Learning/0302-perceptron.html">« 0302-perceptron</a><a class="docs-footer-nextpage" href="0402_Implementing_neural_network_models.html">0402 Implementing neural network models »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.15 on <span class="colophon-date" title="Wednesday 30 March 2022 17:12">Wednesday 30 March 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
